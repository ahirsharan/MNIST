## Objective
### Train LeNet5 on *MNIST* with *SGD* and *Adam* Optimizer and analyze:
  * Effect of training loss vs. Batch size for a fixed learning rate
  * Effect of training loss vs. Learning rate for a fixed Batch size
 
 *This is an assignment submission for the course of Deep Learning Foundations and Applications(AI61002) at IIT Kharagpur.*

