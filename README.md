## Train LeNet5 on *MNIST* with *SGD* and *Adam* Optimizer and analyze:
[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![PyTorch](https://img.shields.io/badge/pytorch-0.4.0-%237732a8)](https://github.com/ahirsharan/MNIST.git)
  * Effect of training loss vs. Batch size for a fixed learning rate
  * Effect of training loss vs. Learning rate for a fixed Batch size
 
 *This is an assignment submission for the course of Deep Learning Foundations and Applications(AI61002) at IIT Kharagpur.*

